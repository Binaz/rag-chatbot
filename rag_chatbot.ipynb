{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLJ7S74Y+HlHLCnanxc+u/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Binaz/rag-chatbot/blob/main/rag_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Energy & Sustainability RAG Chatbot – Development Notebook\n",
        "This notebook demonstrates the development process of a Retrieval-Augmented Generation (RAG) chatbot that answers questions using Energy & Sustainability research papers.\n",
        "\n",
        "**Contents:**\n",
        "1. Loading and extracting text from PDFs\n",
        "2. Chunking text for embedding\n",
        "3. Creating embeddings and FAISS index\n",
        "4. Loading the LLM and setting up the RAG pipeline\n",
        "5. Querying and generating answers\n",
        "6. Testing the Gradio interface\n"
      ],
      "metadata": {
        "id": "yvl1BlgUbAg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Required Packages\n",
        "We use `PyMuPDF` for PDF extraction, `sentence-transformers` for embeddings, `FAISS` for similarity search, and Hugging Face Transformers for LLMs."
      ],
      "metadata": {
        "id": "34EO4xbwbOaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf sentence-transformers faiss-cpu transformers accelerate bitsandbytes gradio torch"
      ],
      "metadata": {
        "id": "xLhZVkRodXi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries\n",
        "Import all necessary libraries for PDF extraction, embeddings, RAG, and Gradio interface.\n"
      ],
      "metadata": {
        "id": "Fmi94Kjzbtiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz, os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss, numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "ftQIEWo9bufm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load PDFs\n",
        "We load research paper PDFs from the `data/pdfs/` folder and extract their text.\n"
      ],
      "metadata": {
        "id": "TGZsIe6kb0vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your folder containing PDFs\n",
        "\n",
        "pdf_folder = \"data/pdfs\"\n",
        "pdf_texts = []\n",
        "for file in os.listdir(pdf_folder):\n",
        "    if file.endswith(\".pdf\"):\n",
        "        path = os.path.join(pdf_folder, file)\n",
        "        doc = fitz.open(path)\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        pdf_texts.append({\"filename\": file, \"text\": text})\n",
        "\n",
        "print(f\"Loaded {len(pdf_texts)} PDFs\")\n"
      ],
      "metadata": {
        "id": "gk7GN2cpb0S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunk Text\n",
        "Split long PDF texts into smaller chunks (~500 words each) for embeddings.\n"
      ],
      "metadata": {
        "id": "KKcEOiqqU83_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunck Text using sentence-transformers/all-MiniLM-L6-v2\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def chunk_text(text, max_tokens=500):     # max_tokens can be 500 - 1000, because rag works best for max_tokens between 500 and 1000.\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), max_tokens):\n",
        "        chunks.append(\" \".join(words[i:i+max_tokens]))\n",
        "    return chunks\n",
        "\n",
        "chunks = []\n",
        "for pdf in pdf_texts:\n",
        "    pdf_chunks = chunk_text(pdf[\"text\"])\n",
        "    for c in pdf_chunks:\n",
        "        chunks.append({\"filename\": pdf[\"filename\"], \"chunk\": c})\n",
        "\n",
        "print(f\"Created {len(chunks)} text chunks\")\n"
      ],
      "metadata": {
        "id": "zNoGoYoYNhOW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Embeddings and FAISS Index\n",
        "Use `all-MiniLM-L6-v2` to encode chunks into vectors and build a FAISS index for semantic search.\n"
      ],
      "metadata": {
        "id": "4vVp1WSrVE5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using 'all-MiniLM-L6-v2' model from huggingFace.\n",
        "# This is a sentence-transformers model and it maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
        "# Using FAISS developed by meta. Faiss is a library for efficient similarity search and clustering of dense vectors.\n",
        "\n",
        "!pip install faiss-cpu sentence-transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create embeddings\n",
        "texts = [c[\"chunk\"] for c in chunks]\n",
        "embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "# Build FAISS index\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(embeddings)\n",
        "\n",
        "print(f\"FAISS index built with {index.ntotal} vectors\")\n"
      ],
      "metadata": {
        "id": "668vCPK8O8Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve Relevant Chunks and Generate Answer\n",
        "Define a function to query the FAISS index and generate answers using the RAG pipeline. Testing a sample query to extract chunks until 650 characters.\n"
      ],
      "metadata": {
        "id": "62IG4GsYVVet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query / Retrieval Function\n",
        "\n",
        "def retrieve(query, top_k=5):\n",
        "    query_emb = model.encode([query])\n",
        "    D, I = index.search(query_emb, top_k)\n",
        "    results = [chunks[i] for i in I[0]]\n",
        "    return results\n",
        "\n",
        "# Test query\n",
        "query = \"Which countries are leading in renewable energy adoption?\"\n",
        "results = retrieve(query)\n",
        "for r in results:\n",
        "    print(r[\"filename\"], r[\"chunk\"][:650], \"...\\n\")\n"
      ],
      "metadata": {
        "id": "n7gU4wNZbPqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load LLM (microsoft/phi-2)\n",
        "We use a small instruct-tuned model from Hugging Face for text generation.\n",
        "\n",
        "> Initally used \"mistralai/Mistral-7B-Instruct-v0.2\" model, but mistralai/Mistral-7B-Instruct-v0.2 needs ≈13–15 GB VRAM, while free Colab T4 gives ~15 GB total, but Colab also uses some of it for notebook kernel.\n",
        "\n",
        "> So it often loads to CPU silently, making inference very slow (hundreds of seconds) or causing no output if the request times out.\n",
        "\n",
        "> So, as a solution to resolve this issue will use \"microsoft/phi-2\" , which is light and faster.\n",
        "\n",
        "\n",
        "> *Average Inference time is 13 seconds.*\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V5p_LL7hVKQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Loading a small instruct-tuned model\n",
        "model_name = \"microsoft/phi-2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "rag_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llm,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=350,\n",
        "    temperature=0.4,      # higher = more diverse\n",
        "    top_p=0.9,            # nucleus sampling\n",
        "    do_sample=False,       # ensures sampling instead of greedy\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "\n",
        "def generate_answer(query, retrieved_chunks, max_chunks=3, max_chunk_tokens=500):\n",
        "    \"\"\"\n",
        "    Generate an answer using retrieved chunks from FAISS.\n",
        "\n",
        "    Args:\n",
        "        query (str): User question\n",
        "        retrieved_chunks (list): List of dicts with 'chunk' and 'filename'\n",
        "        max_chunks (int): Maximum number of chunks to use\n",
        "        max_chunk_tokens (int): Maximum tokens per chunk to prevent context overflow\n",
        "\n",
        "    Returns:\n",
        "        str: Generated answer\n",
        "    \"\"\"\n",
        "    # Keep only top N chunks\n",
        "    retrieved_chunks = retrieved_chunks[:max_chunks]\n",
        "\n",
        "    # Truncate each chunk to max_chunk_tokens tokens\n",
        "    truncated_chunks = []\n",
        "    for c in retrieved_chunks:\n",
        "        tokens = tokenizer(c[\"chunk\"], truncation=True, max_length=max_chunk_tokens)[\"input_ids\"]\n",
        "        truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "        truncated_chunks.append(truncated_text)\n",
        "\n",
        "    # Combine into context\n",
        "    context = \"\\n\\n\".join(truncated_chunks)\n",
        "\n",
        "    # Build prompt\n",
        "    prompt = f\"\"\"You are a helpful research assistant focusing on areas in Energy and Sustainability.\n",
        "Use the context below to answer the question in 6-7 complete sentences\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Generate answer using your LLM pipeline\n",
        "    response = rag_pipeline(prompt)[0][\"generated_text\"]\n",
        "\n",
        "    # Strip prompt repetition if any\n",
        "    if \"Answer:\" in response:\n",
        "        answer = response.split(\"Answer:\")[-1].strip()\n",
        "    else:\n",
        "        answer = response.strip()\n",
        "\n",
        "    return answer\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "E4qxZfajekW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch Chatbot Interface\n",
        "Use Gradio to interact with the RAG chatbot.\n",
        "## Test Queries\n",
        "Try asking some questions about Energy & Sustainability papers."
      ],
      "metadata": {
        "id": "bN5chaAHVtE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def chat_fn(user_input):\n",
        "    # Retrieve top chunks related to the question\n",
        "    retrieved = retrieve(user_input)\n",
        "    print(f\"Retrieved {len(retrieved)} chunks\")  # Debug output in Colab console\n",
        "\n",
        "    # Generate answer using the language model\n",
        "    answer = generate_answer(user_input, retrieved)\n",
        "    print(\"Answer:\", answer)  # Debug output in Colab console\n",
        "\n",
        "    return answer  # Display this in Gradio UI\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=chat_fn,\n",
        "    inputs=gr.Textbox(\n",
        "        lines=2,\n",
        "        placeholder=\"Ask about the research papers on Energy and Sustainability\"\n",
        "    ),\n",
        "    outputs=gr.Textbox(\n",
        "        lines=20,        # height of the output box\n",
        "        max_lines=30     # optional, allows scrolling\n",
        "    ),\n",
        "    title=\"Research Paper RAG Chatbot\"\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "TcxIXUO4h00D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "Ng1DMMdnJWvS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}